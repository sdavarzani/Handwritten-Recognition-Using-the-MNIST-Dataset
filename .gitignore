# Loading basing modules
import tensorflow as tf
import numpy as np

from tensorflow.examples.tutorials.mnist import input_data

# Hyperparameters and constants
second_layer_size = 10
third_layer_size = 10
image_size = 28
label_size = 10
learning_rate = 0.05
steps_number = 7000
batch_size = 100

# Read MNIST data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

# Define placeholders
training_data = tf.placeholder(tf.float32, [None, None])
labels = tf.placeholder(tf.float32, [None, None])

# Second layer parameters
W1 = tf.Variable(tf.truncated_normal([image_size*image_size, second_layer_size], stddev=0.1))
b1 = tf.Variable(tf.truncated_normal(shape=[second_layer_size], stddev=0.1))

# Third layer parameters
W2 = tf.Variable(tf.truncated_normal([second_layer_size, third_layer_size], stddev=0.1))
b2 = tf.Variable(tf.truncated_normal(shape=[third_layer_size], stddev=0.1))

# Output layer parameters
W3 = tf.Variable(tf.truncated_normal([third_layer_size, label_size], stddev=0.1))
b3 = tf.Variable(tf.truncated_normal(shape=[label_size], stddev=0.1))

# Create output of layers one after the other
h1 = tf.math.sigmoid(tf.linalg.matmul(training_data, W1)+b1)
h2 = tf.math.sigmoid(tf.linalg.matmul(h1, W2)+b2)
# Create output with sigmoid activation function
output1 = tf.linalg.matmul(h2, W3)+b3
# Create output with identity activation function
output2 = tf.math.sigmoid(tf.linalg.matmul(h2, W3)+b3)

# Define loss function for two outputs 
loss1 = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(labels, output1))))
loss2 = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(labels, output2))))

# Training step
train_step1 = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss1)
train_step2 = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss2)

# Accuracy calculation
correct_prediction1 = tf.equal(tf.argmax(output1, 1), tf.argmax(labels, 1))
accuracy1 = tf.reduce_mean(tf.cast(correct_prediction1, tf.float32))

correct_prediction2 = tf.equal(tf.argmax(output2, 1), tf.argmax(labels, 1))
accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32))

# Run the training for MNIST data
with tf.Session() as sess:
  init=tf.global_variables_initializer()
  sess.run(init)
  for i in range(steps_number):
    # Get the next batch
    input_batch, labels_batch = mnist.train.next_batch(batch_size)
    feed_dict = {training_data: input_batch, labels: labels_batch}

    # Run the training step
    train_step1.run(feed_dict=feed_dict)
    train_step2.run(feed_dict=feed_dict)

    # Print the accuracy progress on the batch every 100 steps
    if i%100 == 0:
      train_accuracy1 = accuracy1.eval(feed_dict=feed_dict)
      print("Step %d, training batch accuracy for MLP with identity output layer %g %%"%(i, train_accuracy1*100))

      train_accuracy2 = accuracy2.eval(feed_dict=feed_dict)
      print("          training batch accuracy for MLP with sigmoid output layer %g %%"%(train_accuracy2*100))

  # Evaluate on the test set
  test_accuracy1 = accuracy1.eval(feed_dict={training_data: mnist.test.images, labels: mnist.test.labels})
  print("Test loss for MLP with identity output layer: %g "%(test_accuracy1))

  test_accuracy2 = accuracy2.eval(feed_dict={training_data: mnist.test.images, labels: mnist.test.labels})
  print("Test accuracy for MLP with sigmoid output layer: %g"%(test_accuracy2))
  sess.close()
